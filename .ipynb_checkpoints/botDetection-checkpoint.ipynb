{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fbf327",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_train.csv\")\n",
    "val_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_validation.csv\")\n",
    "test_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424add00",
   "metadata": {},
   "source": [
    "<h2>Data Exploration</h2>\n",
    "\n",
    "In this section we look at the structure of the dataset after the extraction of the tweets using the twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6783dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "      <th>screen_name_anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1110407881030017024</td>\n",
       "      <td>1208265880146046976</td>\n",
       "      <td>imranyebot</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3171109449</td>\n",
       "      <td>1091463908118941696</td>\n",
       "      <td>zawvrk</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>human#17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110686081341632512</td>\n",
       "      <td>1199055191028293633</td>\n",
       "      <td>zawarbot</td>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1110307772783124480</td>\n",
       "      <td>1214698264701722626</td>\n",
       "      <td>ahadsheriffbot</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979586167405363200</td>\n",
       "      <td>1209229478934695937</td>\n",
       "      <td>kevinhookebot</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "      <td>bot#11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id            status_id     screen_name  \\\n",
       "0  1110407881030017024  1208265880146046976      imranyebot   \n",
       "1           3171109449  1091463908118941696          zawvrk   \n",
       "2  1110686081341632512  1199055191028293633        zawarbot   \n",
       "3  1110307772783124480  1214698264701722626  ahadsheriffbot   \n",
       "4   979586167405363200  1209229478934695937   kevinhookebot   \n",
       "\n",
       "                                                text account.type class_type  \\\n",
       "0                             YEA now that note GOOD          bot     others   \n",
       "1  Listen to This Charming Man by The Smiths  htt...        human      human   \n",
       "2  wish i can i would be seeing other hoes on the...          bot     others   \n",
       "3  The decade in the significantly easier schedul...          bot     others   \n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot        rnn   \n",
       "\n",
       "  screen_name_anonymized  \n",
       "0                  bot#9  \n",
       "1               human#17  \n",
       "2                 bot#23  \n",
       "3                  bot#1  \n",
       "4                 bot#11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce001b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20712 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "nRows,nCols = train_df.shape\n",
    "print(f\"There are {nRows} rows and {nCols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd0e5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['account.type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cef5986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "      <th>screen_name_anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>41</td>\n",
       "      <td>20710</td>\n",
       "      <td>40</td>\n",
       "      <td>20712</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>979586167405363200</td>\n",
       "      <td>1279906540791779334</td>\n",
       "      <td>kevinhooke</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>human#10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1951</td>\n",
       "      <td>2</td>\n",
       "      <td>1951</td>\n",
       "      <td>1</td>\n",
       "      <td>10358</td>\n",
       "      <td>10358</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id            status_id screen_name  \\\n",
       "count                20712                20712       20712   \n",
       "unique                  41                20710          40   \n",
       "top     979586167405363200  1279906540791779334  kevinhooke   \n",
       "freq                  1951                    2        1951   \n",
       "\n",
       "                          text account.type class_type screen_name_anonymized  \n",
       "count                    20712        20712      20712                  20712  \n",
       "unique                   20712            2          4                     40  \n",
       "top     YEA now that note GOOD        human      human               human#10  \n",
       "freq                         1        10358      10358                   1951  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37af88e",
   "metadata": {},
   "source": [
    "<h2>Data Pre-Processing</h2>\n",
    "\n",
    "<p>The first task is to remove extra columns. Since we will only be using the 'text' and 'account.type' columns, we will copy them to a new dataframe.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abe32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85771264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text account.type\n",
       "0                             YEA now that note GOOD          bot\n",
       "1  Listen to This Charming Man by The Smiths  htt...        human\n",
       "2  wish i can i would be seeing other hoes on the...          bot\n",
       "3  The decade in the significantly easier schedul...          bot\n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e76b5d",
   "metadata": {},
   "source": [
    "We now deal with hashtags, URLS, Mentions, Emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50722648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>[yea, note, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>[listen, charm, man, smith, __url__]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[wish, would, see, hoe, worst, part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[decad, significantli, easier, schedul, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[theim, class, =\\, alignnon, size-ful, wp-imag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text account.type  \\\n",
       "0                             YEA now that note GOOD          bot   \n",
       "1  Listen to This Charming Man by The Smiths  htt...        human   \n",
       "2  wish i can i would be seeing other hoes on the...          bot   \n",
       "3  The decade in the significantly easier schedul...          bot   \n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                                  [yea, note, good]  \n",
       "1               [listen, charm, man, smith, __url__]  \n",
       "2               [wish, would, see, hoe, worst, part]  \n",
       "3  [decad, significantli, easier, schedul, like, ...  \n",
       "4  [theim, class, =\\, alignnon, size-ful, wp-imag...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    twt = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    # combine stop words and punctuation\n",
    "    stop = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    # create the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tokens = [ lemmatizer.lemmatize(stemmer.stem(token))\n",
    "              for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "        allText = \" \".join(tokens)\n",
    "        allText = re.sub(r'http\\S+', '__url__', allText)\n",
    "        tokens = allText.split()\n",
    "        tokenized_tweets.append(tokens)\n",
    "    return tokenized_tweets\n",
    "\n",
    "train_df['cleaned_text'] = tokenizeTweets(train_df['text'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57edaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)\n",
    "val_df['cleaned_text'] = tokenizeTweets(val_df['text'])\n",
    "\n",
    "test_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)\n",
    "test_df['cleaned_text'] = tokenizeTweets(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd29bbc",
   "metadata": {},
   "source": [
    "<h2>Modeling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b1695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = train_df['cleaned_text'].tolist()\n",
    "X_train = [\" \".join(i) for i in X_train]\n",
    "\n",
    "X_val = val_df['cleaned_text'].tolist()\n",
    "X_val = [\" \".join(i) for i in X_val]\n",
    "\n",
    "X_test = test_df['cleaned_text'].tolist()\n",
    "X_test = [\" \".join(i) for i in X_test]\n",
    "\n",
    "dictLabels = {\"human\":0, \"bot\":1}\n",
    "dictLabelsReverse = {0:\"human\", 1: \"bot\"}\n",
    "\n",
    "y_train = train_df[\"account.type\"].apply(lambda x: dictLabels[x])\n",
    "y_val = val_df[\"account.type\"].apply(lambda x: dictLabels[x])\n",
    "y_test = test_df[\"account.type\"].apply(lambda x: dictLabels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c15cf15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yea</th>\n",
       "      <td>0.688537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>note</th>\n",
       "      <td>0.607476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.396093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petti</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excless</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclass</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitedli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>željko</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20872 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "yea        0.688537\n",
       "note       0.607476\n",
       "good       0.396093\n",
       "00         0.000000\n",
       "petti      0.000000\n",
       "...             ...\n",
       "excless    0.000000\n",
       "exclass    0.000000\n",
       "excitedli  0.000000\n",
       "excit      0.000000\n",
       "željko     0.000000\n",
       "\n",
       "[20872 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1, 1), max_features=25000, \n",
    "                       dtype=np.float32)\n",
    "train_features = vect.fit_transform(X_train)\n",
    "valid_features = vect.transform(X_val)\n",
    "test_features = vect.transform(X_test)\n",
    "\n",
    "\n",
    "first_vector_tfidfvectorizer = train_features[0]\n",
    "dfVec = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vect.get_feature_names(), columns=[\"tfidf\"])\n",
    "dfVec.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8938b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y_train.tolist()\n",
    "val_labels = y_val.tolist()\n",
    "test_labels = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83c57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(estimator=SVC(),\n",
      "             param_grid=[{'C': [1, 10, 100], 'gamma': [0.001, 0.0001],\n",
      "                          'kernel': ['linear', 'rbf']}])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Test SVC.\n",
    "parameters = [{'kernel': ['linear','rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                      'C': [1, 10, 100]}]\n",
    "                    #  {'kernel': ['linear'], 'C': [1]}]\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "tuning_results = clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce8c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72      1278\n",
      "           1       0.71      0.83      0.77      1280\n",
      "\n",
      "    accuracy                           0.75      2558\n",
      "   macro avg       0.75      0.75      0.75      2558\n",
      "weighted avg       0.75      0.75      0.75      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(clf.best_params_) \n",
    "grid_predictions = clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2260355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73      1278\n",
      "           1       0.72      0.80      0.76      1280\n",
      "\n",
      "    accuracy                           0.74      2558\n",
      "   macro avg       0.75      0.74      0.74      2558\n",
      "weighted avg       0.75      0.74      0.74      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Test logistic regression.\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "parameters = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, parameters)\n",
    "lr_clf.fit(train_features,train_labels)\n",
    "print(lr_clf.best_params_) \n",
    "grid_predictions = lr_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46c563c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "{'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70      1278\n",
      "           1       0.69      0.82      0.75      1280\n",
      "\n",
      "    accuracy                           0.73      2558\n",
      "   macro avg       0.74      0.73      0.73      2558\n",
      "weighted avg       0.74      0.73      0.73      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "random_state = 523\n",
    "n_estimators = [100, 300, 500]\n",
    "max_depth = [5, 15, 30]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [2, 5, 10] \n",
    "\n",
    "parameters = {'n_estimators' : n_estimators, 'max_depth' : max_depth,  \n",
    "              'min_samples_split' : min_samples_split, \n",
    "            'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(rf.get_params().keys())\n",
    "rf_clf = GridSearchCV(rf, parameters)\n",
    "rf_clf.fit(train_features,train_labels)\n",
    "print(rf_clf.best_params_) \n",
    "grid_predictions = rf_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "078b6ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3d3437d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size)).to('cuda') #hidden state\n",
    "        c_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size)).to('cuda') #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        #out = self.l1(x)\n",
    "        #out = self.relu(hn)\n",
    "        out = self.linear(hn)\n",
    "        #out = self.relu(out)\n",
    "        #out = self.l3(out)\n",
    "        \n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e54b8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy\n",
    "\n",
    "\"\"\"x_train = torch.tensor(scipy.sparse.csr_matrix.todense(train_features)).float()\n",
    "y_train = torch.tensor(train_labels)\n",
    "\n",
    "x_val = torch.tensor(scipy.sparse.csr_matrix.todense(valid_features)).float()\n",
    "y_val = torch.tensor(val_labels)\n",
    "\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(test_features)).float()\n",
    "y_test = torch.tensor(test_labels)\"\"\"\n",
    "\n",
    "class TrainTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = x_train.shape[0]\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ValTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = x_val.shape[0]\n",
    "        self.x_data = x_val\n",
    "        self.y_data = y_val\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class TestTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = x_test.shape[0]\n",
    "        self.x_data = x_test\n",
    "        self.y_data = y_test\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "train_dataset = TrainTweetDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "val_dataset = ValTweetDataset()\n",
    "valid_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "test_dataset = TestTweetDataset()\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01feb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "def fitLSTM():\n",
    "    train_losses=[]\n",
    "    train_accu=[]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTM(input_size, hidden_size, num_classes).to(device)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        correct=0\n",
    "        total=0\n",
    "        running_loss=0\n",
    "        for (words, labels) in train_loader:\n",
    "            words = words.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            # if y would be one-hot, we must apply\n",
    "            #labels = torch.max(labels, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            running_accuracy = 0 \n",
    "            total = 0 \n",
    "    \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss=running_loss/len(train_loader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "        print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "            \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        for data, labels in valid_loader:\n",
    "            # Transfer Data to GPU if available\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward Pass\n",
    "            target = model(data.float())\n",
    "            # Find the Loss\n",
    "            loss = criterion(target,labels)\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        valid_loss = running_loss/len(valid_loader)\n",
    "        print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "        \n",
    "    print(f'final loss: {loss.item():.4f}')\n",
    "    \n",
    "    path = \"saved_model.pth\" \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "     \n",
    "    running_accuracy = 0 \n",
    "    total = 0 \n",
    " \n",
    "    with torch.no_grad(): \n",
    "        for inputs, outputs in test_loader: \n",
    "            if torch.cuda.is_available():\n",
    "                inputs, outputs = inputs.cuda(), outputs.cuda()\n",
    "            \n",
    "            outputs = outputs.to(torch.float32) \n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "            total += outputs.size(0) \n",
    "            running_accuracy += (predicted == outputs).sum().item() \n",
    " \n",
    "        \n",
    "    print('Accuracy of the model based on the test set of inputs is: %d %%' % (100 * running_accuracy / total))    \n",
    "    \n",
    "    labels_length = 2\n",
    "    labels_correct =[0. for i in range(labels_length)] # list to calculate correct labels [how many correct setosa, how many correct versicolor, how many correct virginica] \n",
    "    labels_total = [0. for i in range(labels_length)]   # list to keep the total # of labels per type [total setosa, total versicolor, total virginica] \n",
    "  \n",
    "    with torch.no_grad(): \n",
    "         for inputs, outputs in test_loader: \n",
    "            if torch.cuda.is_available():\n",
    "                inputs, outputs = inputs.cuda(), outputs.cuda()\n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "             \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == outputs[i]:\n",
    "                    labels_correct[outputs[i]] += 1\n",
    "                labels_total[outputs[i]] += 1\n",
    "            \"\"\"            label_correct_running = (predicted == outputs).squeeze()\n",
    "                        label = outputs[0]\n",
    "                        print(label)\n",
    "                        print(predicted)\n",
    "                        print(outputs)\n",
    "                        print(label_correct_running)\n",
    "                        if label_correct_running.item():  \n",
    "                            labels_correct[label] += 1 \n",
    "                        labels_total[label] += 1  \n",
    "            \"\"\"  \n",
    "    #label_list = list(labels.keys()) \n",
    "    for i in range(2):\n",
    "        print('Accuracy to predict %5s : %2d %%' % (i, 100 * labels_correct[i] / labels_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc2ba049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.476 | Accuracy: 100.000\n",
      "Epoch 1 \t\t Training Loss: 0.47594544701323294 \t\t Validation Loss: 0.47131021412009333\n",
      "Validation Loss Decreased(inf--->0.471310) \t Saving The Model\n",
      "Train Loss: 0.352 | Accuracy: 100.000\n",
      "Epoch 2 \t\t Training Loss: 0.3519033737039092 \t\t Validation Loss: 0.4911036998442266\n",
      "Train Loss: 0.243 | Accuracy: 100.000\n",
      "Epoch 3 \t\t Training Loss: 0.2430733447592701 \t\t Validation Loss: 0.6316833073903311\n",
      "Train Loss: 0.142 | Accuracy: 100.000\n",
      "Epoch 4 \t\t Training Loss: 0.14241140952402465 \t\t Validation Loss: 1.1310761876981128\n",
      "Train Loss: 0.082 | Accuracy: 100.000\n",
      "Epoch 5 \t\t Training Loss: 0.08165801600680028 \t\t Validation Loss: 1.5081042138198006\n",
      "Train Loss: 0.050 | Accuracy: 100.000\n",
      "Epoch 6 \t\t Training Loss: 0.04953727712251266 \t\t Validation Loss: 2.113017996433214\n",
      "Train Loss: 0.039 | Accuracy: 100.000\n",
      "Epoch 7 \t\t Training Loss: 0.03857009017188232 \t\t Validation Loss: 2.2469994465221044\n",
      "Train Loss: 0.031 | Accuracy: 100.000\n",
      "Epoch 8 \t\t Training Loss: 0.031100598164433266 \t\t Validation Loss: 2.3472163595361377\n",
      "Train Loss: 0.026 | Accuracy: 100.000\n",
      "Epoch 9 \t\t Training Loss: 0.026492774836583445 \t\t Validation Loss: 2.368770511880484\n",
      "Train Loss: 0.024 | Accuracy: 100.000\n",
      "Epoch 10 \t\t Training Loss: 0.023627564588577715 \t\t Validation Loss: 2.5005665444389176\n",
      "final loss: 30.3834\n",
      "Accuracy of the model based on the test set of inputs is: 75 %\n",
      "Accuracy to predict     0 : 60 %\n",
      "Accuracy to predict     1 : 90 %\n"
     ]
    }
   ],
   "source": [
    "fitLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07065cb7",
   "metadata": {},
   "source": [
    "<h1> BERT Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45d986ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->100\n",
      "->200\n",
      "->300\n",
      "->400\n",
      "->500\n",
      "->600\n",
      "->700\n",
      "->800\n",
      "->900\n",
      "->1000\n",
      "->1100\n",
      "->1200\n",
      "->1300\n",
      "->1400\n",
      "->1500\n",
      "->1600\n",
      "->1700\n",
      "->1800\n",
      "->1900\n",
      "->2000\n",
      "->2100\n",
      "->2200\n",
      "->2300\n",
      "->2400\n",
      "->2500\n",
      "->2600\n",
      "->2700\n",
      "->2800\n",
      "->2900\n",
      "->3000\n",
      "->3100\n",
      "->3200\n",
      "->3300\n",
      "->3400\n",
      "->3500\n",
      "->3600\n",
      "->3700\n",
      "->3800\n",
      "->3900\n",
      "->4000\n",
      "->4100\n",
      "->4200\n",
      "->4300\n",
      "->4400\n",
      "->4500\n",
      "->4600\n",
      "->4700\n",
      "->4800\n",
      "->4900\n",
      "->5000\n",
      "->5100\n",
      "->5200\n",
      "->5300\n",
      "->5400\n",
      "->5500\n",
      "->5600\n",
      "->5700\n",
      "->5800\n",
      "->5900\n",
      "->6000\n",
      "->6100\n",
      "->6200\n",
      "->6300\n",
      "->6400\n",
      "->6500\n",
      "->6600\n",
      "->6700\n",
      "->6800\n",
      "->6900\n",
      "->7000\n",
      "->7100\n",
      "->7200\n",
      "->7300\n",
      "->7400\n",
      "->7500\n",
      "->7600\n",
      "->7700\n",
      "->7800\n",
      "->7900\n",
      "->8000\n",
      "->8100\n",
      "->8200\n",
      "->8300\n",
      "->8400\n",
      "->8500\n",
      "->8600\n",
      "->8700\n",
      "->8800\n",
      "->8900\n",
      "->9000\n",
      "->9100\n",
      "->9200\n",
      "->9300\n",
      "->9400\n",
      "->9500\n",
      "->9600\n",
      "->9700\n",
      "->9800\n",
      "->9900\n",
      "->10000\n",
      "->10100\n",
      "->10200\n",
      "->10300\n",
      "->10400\n",
      "->10500\n",
      "->10600\n",
      "->10700\n",
      "->10800\n",
      "->10900\n",
      "->11000\n",
      "->11100\n",
      "->11200\n",
      "->11300\n",
      "->11400\n",
      "->11500\n",
      "->11600\n",
      "->11700\n",
      "->11800\n",
      "->11900\n",
      "->12000\n",
      "->12100\n",
      "->12200\n",
      "->12300\n",
      "->12400\n",
      "->12500\n",
      "->12600\n",
      "->12700\n",
      "->12800\n",
      "->12900\n",
      "->13000\n",
      "->13100\n",
      "->13200\n",
      "->13300\n",
      "->13400\n",
      "->13500\n",
      "->13600\n",
      "->13700\n",
      "->13800\n",
      "->13900\n",
      "->14000\n",
      "->14100\n",
      "->14200\n",
      "->14300\n",
      "->14400\n",
      "->14500\n",
      "->14600\n",
      "->14700\n",
      "->14800\n",
      "->14900\n",
      "->15000\n",
      "->15100\n",
      "->15200\n",
      "->15300\n",
      "->15400\n",
      "->15500\n",
      "->15600\n",
      "->15700\n",
      "->15800\n",
      "->15900\n",
      "->16000\n",
      "->16100\n",
      "->16200\n",
      "->16300\n",
      "->16400\n",
      "->16500\n",
      "->16600\n",
      "->16700\n",
      "->16800\n",
      "->16900\n",
      "->17000\n",
      "->17100\n",
      "->17200\n",
      "->17300\n",
      "->17400\n",
      "->17500\n",
      "->17600\n",
      "->17700\n",
      "->17800\n",
      "->17900\n",
      "->18000\n",
      "->18100\n",
      "->18200\n",
      "->18300\n",
      "->18400\n",
      "->18500\n",
      "->18600\n",
      "->18700\n",
      "->18800\n",
      "->18900\n",
      "->19000\n",
      "->19100\n",
      "->19200\n",
      "->19300\n",
      "->19400\n",
      "->19500\n",
      "->19600\n",
      "->19700\n",
      "->19800\n",
      "->19900\n",
      "->20000\n",
      "->20100\n",
      "->20200\n",
      "->20300\n",
      "->20400\n",
      "->20500\n",
      "->20600\n",
      "->20700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->100\n",
      "->200\n",
      "->300\n",
      "->400\n",
      "->500\n",
      "->600\n",
      "->700\n",
      "->800\n",
      "->900\n",
      "->1000\n",
      "->1100\n",
      "->1200\n",
      "->1300\n",
      "->1400\n",
      "->1500\n",
      "->1600\n",
      "->1700\n",
      "->1800\n",
      "->1900\n",
      "->2000\n",
      "->2100\n",
      "->2200\n",
      "->2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->100\n",
      "->200\n",
      "->300\n",
      "->400\n",
      "->500\n",
      "->600\n",
      "->700\n",
      "->800\n",
      "->900\n",
      "->1000\n",
      "->1100\n",
      "->1200\n",
      "->1300\n",
      "->1400\n",
      "->1500\n",
      "->1600\n",
      "->1700\n",
      "->1800\n",
      "->1900\n",
      "->2000\n",
      "->2100\n",
      "->2200\n",
      "->2300\n",
      "->2400\n",
      "->2500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "\n",
    "    \n",
    "def BERTtokenizeTweets(df):\n",
    "    pretrained_weights = \"bert-base-cased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = BertModel.from_pretrained(pretrained_weights)\n",
    "    nlp = pipeline(\"feature-extraction\", tokenizer=tokenizer, model=model)\n",
    "    vectorized_docs = []\n",
    "    count = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Encode text\n",
    "        vec = np.array(nlp(row[\"text\"][:512]))\n",
    "        meanVec = vec.reshape((vec.shape[1], vec.shape[2])).mean(axis=0)\n",
    "        vectorized_docs.append(meanVec)\n",
    "        count = count + 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"->\"+str(count)) \n",
    "\n",
    "    return np.array(vectorized_docs)\n",
    "\n",
    "\n",
    "train_features = BERTtokenizeTweets(train_df)\n",
    "val_features = BERTtokenizeTweets(val_df)\n",
    "test_features = BERTtokenizeTweets(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fb1bd2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      1278\n",
      "           1       0.83      0.86      0.85      1280\n",
      "\n",
      "    accuracy                           0.84      2558\n",
      "   macro avg       0.84      0.84      0.84      2558\n",
      "weighted avg       0.84      0.84      0.84      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Test SVC.\n",
    "parameters = [#{'kernel': ['linear','rbf'], 'gamma': [1e-3, 1e-4],\n",
    "              #        'C': [1, 10, 100]}]\n",
    "                      {'kernel': ['linear'], 'C': [1]}]\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "tuning_results = clf.fit(train_features,train_labels)\n",
    "\n",
    "print(clf.best_params_) \n",
    "grid_predictions = clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6478c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84      1278\n",
      "           1       0.83      0.85      0.84      1280\n",
      "\n",
      "    accuracy                           0.84      2558\n",
      "   macro avg       0.84      0.84      0.84      2558\n",
      "weighted avg       0.84      0.84      0.84      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Test logistic regression.\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "parameters = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, parameters)\n",
    "lr_clf.fit(train_features,train_labels)\n",
    "print(lr_clf.best_params_) \n",
    "grid_predictions = lr_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4b04b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "{'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82      1278\n",
      "           1       0.80      0.87      0.83      1280\n",
      "\n",
      "    accuracy                           0.83      2558\n",
      "   macro avg       0.83      0.83      0.82      2558\n",
      "weighted avg       0.83      0.83      0.82      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "random_state = 523\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [15, 30]\n",
    "min_samples_split = [10, 15]\n",
    "min_samples_leaf = [ 5, 10] \n",
    "\n",
    "parameters = {'n_estimators' : n_estimators, 'max_depth' : max_depth,  \n",
    "              'min_samples_split' : min_samples_split, \n",
    "            'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(rf.get_params().keys())\n",
    "rf_clf = GridSearchCV(rf, parameters)\n",
    "rf_clf.fit(train_features,train_labels)\n",
    "print(rf_clf.best_params_) \n",
    "grid_predictions = rf_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0bd3d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy\n",
    "\n",
    "#x_trainBERT = torch.tensor(scipy.sparse.csr_matrix.todense(train_features)).float()\n",
    "#y_trainBERT = torch.tensor(train_labels)\n",
    "\n",
    "#x_valBERT = torch.tensor(scipy.sparse.csr_matrix.todense(valid_features)).float()\n",
    "#y_valBERT = torch.tensor(val_labels)\n",
    "\n",
    "#x_testBERT = torch.tensor(scipy.sparse.csr_matrix.todense(test_features)).float()\n",
    "#y_testBERT = torch.tensor(test_labels)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "class TrainTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = train_features.shape[0]\n",
    "        self.x_data = train_features\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ValTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = valid_features.shape[0]\n",
    "        self.x_data = valid_features\n",
    "        self.y_data = y_val\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class TestTweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = test_features.shape[0]\n",
    "        self.x_data = test_features\n",
    "        self.y_data = y_test\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "BERTtrain_dataset = TrainTweetDataset()\n",
    "BERTtrain_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "BERTval_dataset = ValTweetDataset()\n",
    "BERTvalid_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "BERTtest_dataset = TestTweetDataset()\n",
    "BERTtest_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c5225941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLSTM():\n",
    "    train_losses=[]\n",
    "    train_accu=[]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTM(input_size, hidden_size, num_classes).to(device)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        correct=0\n",
    "        total=0\n",
    "        running_loss=0\n",
    "        for (words, labels) in BERTtrain_loader:\n",
    "            words = words.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            # if y would be one-hot, we must apply\n",
    "            #labels = torch.max(labels, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            running_accuracy = 0 \n",
    "            total = 0 \n",
    "    \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss=running_loss/len(BERTtrain_loader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "        print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "            \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        for data, labels in BERTvalid_loader:\n",
    "            # Transfer Data to GPU if available\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward Pass\n",
    "            target = model(data.float())\n",
    "            # Find the Loss\n",
    "            loss = criterion(target,labels)\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        valid_loss = running_loss/len(BERTvalid_loader)\n",
    "        print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "        \n",
    "    print(f'final loss: {loss.item():.4f}')\n",
    "    \n",
    "    path = \"saved_model.pth\" \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "     \n",
    "    running_accuracy = 0 \n",
    "    total = 0 \n",
    " \n",
    "    with torch.no_grad(): \n",
    "        for inputs, outputs in BERTtest_loader: \n",
    "            if torch.cuda.is_available():\n",
    "                inputs, outputs = inputs.cuda(), outputs.cuda()\n",
    "            \n",
    "            outputs = outputs.to(torch.float32) \n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "            total += outputs.size(0) \n",
    "            running_accuracy += (predicted == outputs).sum().item() \n",
    " \n",
    "        \n",
    "    print('Accuracy of the model based on the test set of inputs is: %d %%' % (100 * running_accuracy / total))    \n",
    "    \n",
    "    labels_length = 2\n",
    "    labels_correct =[0. for i in range(labels_length)] # list to calculate correct labels [how many correct setosa, how many correct versicolor, how many correct virginica] \n",
    "    labels_total = [0. for i in range(labels_length)]   # list to keep the total # of labels per type [total setosa, total versicolor, total virginica] \n",
    "  \n",
    "    with torch.no_grad(): \n",
    "         for inputs, outputs in BERTtest_loader: \n",
    "            if torch.cuda.is_available():\n",
    "                inputs, outputs = inputs.cuda(), outputs.cuda()\n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "             \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == outputs[i]:\n",
    "                    labels_correct[outputs[i]] += 1\n",
    "                labels_total[outputs[i]] += 1\n",
    "            \"\"\"            label_correct_running = (predicted == outputs).squeeze()\n",
    "                        label = outputs[0]\n",
    "                        print(label)\n",
    "                        print(predicted)\n",
    "                        print(outputs)\n",
    "                        print(label_correct_running)\n",
    "                        if label_correct_running.item():  \n",
    "                            labels_correct[label] += 1 \n",
    "                        labels_total[label] += 1  \n",
    "            \"\"\"  \n",
    "    #label_list = list(labels.keys()) \n",
    "    for i in range(2):\n",
    "        print('Accuracy to predict %5s : %2d %%' % (i, 100 * labels_correct[i] / labels_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eefc0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.478 | Accuracy: 0.000\n",
      "Epoch 1 \t\t Training Loss: 0.4778758283249783 \t\t Validation Loss: 0.43995358429263726\n",
      "Validation Loss Decreased(inf--->0.439954) \t Saving The Model\n",
      "Train Loss: 0.356 | Accuracy: 100.000\n",
      "Epoch 2 \t\t Training Loss: 0.3563181662422664 \t\t Validation Loss: 0.46961406537022515\n",
      "Train Loss: 0.255 | Accuracy: 100.000\n",
      "Epoch 3 \t\t Training Loss: 0.2546845307406574 \t\t Validation Loss: 0.6434946088412685\n",
      "Train Loss: 0.163 | Accuracy: 100.000\n",
      "Epoch 4 \t\t Training Loss: 0.16328676839362577 \t\t Validation Loss: 1.0439164729797732\n",
      "Train Loss: 0.097 | Accuracy: 100.000\n",
      "Epoch 5 \t\t Training Loss: 0.09678650153352103 \t\t Validation Loss: 1.635250535339905\n",
      "Train Loss: 0.061 | Accuracy: 100.000\n",
      "Epoch 6 \t\t Training Loss: 0.06067621327237012 \t\t Validation Loss: 2.121353747409521\n",
      "Train Loss: 0.043 | Accuracy: 100.000\n",
      "Epoch 7 \t\t Training Loss: 0.043246145312298374 \t\t Validation Loss: 2.237136697821773\n",
      "Train Loss: 0.034 | Accuracy: 100.000\n",
      "Epoch 8 \t\t Training Loss: 0.03394001540453645 \t\t Validation Loss: 2.4966590037784986\n",
      "Train Loss: 0.029 | Accuracy: 100.000\n",
      "Epoch 9 \t\t Training Loss: 0.029343552060122166 \t\t Validation Loss: 2.637618827379524\n",
      "Train Loss: 0.025 | Accuracy: 100.000\n",
      "Epoch 10 \t\t Training Loss: 0.02464952353976773 \t\t Validation Loss: 2.807385726801919\n",
      "final loss: 0.0000\n",
      "Accuracy of the model based on the test set of inputs is: 75 %\n",
      "Accuracy to predict     0 : 65 %\n",
      "Accuracy to predict     1 : 84 %\n"
     ]
    }
   ],
   "source": [
    "fitLSTM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
