{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fbf327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_train.csv\")\n",
    "val_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_validation.csv\")\n",
    "test_df = pd.read_csv(\".\\\\tweepfake_dataset\\\\full_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424add00",
   "metadata": {},
   "source": [
    "<h2>Data Exploration</h2>\n",
    "\n",
    "In this section we look at the structure of the dataset after the extraction of the tweets using the twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6783dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "      <th>screen_name_anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1110407881030017024</td>\n",
       "      <td>1208265880146046976</td>\n",
       "      <td>imranyebot</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3171109449</td>\n",
       "      <td>1091463908118941696</td>\n",
       "      <td>zawvrk</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>human#17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110686081341632512</td>\n",
       "      <td>1199055191028293633</td>\n",
       "      <td>zawarbot</td>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1110307772783124480</td>\n",
       "      <td>1214698264701722626</td>\n",
       "      <td>ahadsheriffbot</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>bot#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979586167405363200</td>\n",
       "      <td>1209229478934695937</td>\n",
       "      <td>kevinhookebot</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "      <td>bot#11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id            status_id     screen_name  \\\n",
       "0  1110407881030017024  1208265880146046976      imranyebot   \n",
       "1           3171109449  1091463908118941696          zawvrk   \n",
       "2  1110686081341632512  1199055191028293633        zawarbot   \n",
       "3  1110307772783124480  1214698264701722626  ahadsheriffbot   \n",
       "4   979586167405363200  1209229478934695937   kevinhookebot   \n",
       "\n",
       "                                                text account.type class_type  \\\n",
       "0                             YEA now that note GOOD          bot     others   \n",
       "1  Listen to This Charming Man by The Smiths  htt...        human      human   \n",
       "2  wish i can i would be seeing other hoes on the...          bot     others   \n",
       "3  The decade in the significantly easier schedul...          bot     others   \n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot        rnn   \n",
       "\n",
       "  screen_name_anonymized  \n",
       "0                  bot#9  \n",
       "1               human#17  \n",
       "2                 bot#23  \n",
       "3                  bot#1  \n",
       "4                 bot#11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce001b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20712 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "nRows,nCols = train_df.shape\n",
    "print(f\"There are {nRows} rows and {nCols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd0e5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from this we can see that the dataset is balanced\n",
    "train_df['account.type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cef5986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "      <th>screen_name_anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "      <td>20712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>41</td>\n",
       "      <td>20710</td>\n",
       "      <td>40</td>\n",
       "      <td>20712</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>979586167405363200</td>\n",
       "      <td>1279906540791779334</td>\n",
       "      <td>kevinhooke</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>human#10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1951</td>\n",
       "      <td>2</td>\n",
       "      <td>1951</td>\n",
       "      <td>1</td>\n",
       "      <td>10358</td>\n",
       "      <td>10358</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id            status_id screen_name  \\\n",
       "count                20712                20712       20712   \n",
       "unique                  41                20710          40   \n",
       "top     979586167405363200  1279906540791779334  kevinhooke   \n",
       "freq                  1951                    2        1951   \n",
       "\n",
       "                          text account.type class_type screen_name_anonymized  \n",
       "count                    20712        20712      20712                  20712  \n",
       "unique                   20712            2          4                     40  \n",
       "top     YEA now that note GOOD        human      human               human#10  \n",
       "freq                         1        10358      10358                   1951  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic stats about the dataset\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37af88e",
   "metadata": {},
   "source": [
    "<h2>Data Pre-Processing</h2>\n",
    "\n",
    "<p>The first task is to remove extra columns. Since we will only be using the 'text' and 'account.type' columns, we will copy them to a new dataframe.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abe32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85771264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text account.type\n",
       "0                             YEA now that note GOOD          bot\n",
       "1  Listen to This Charming Man by The Smiths  htt...        human\n",
       "2  wish i can i would be seeing other hoes on the...          bot\n",
       "3  The decade in the significantly easier schedul...          bot\n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e76b5d",
   "metadata": {},
   "source": [
    "We now deal with hashtags, URLS, Mentions, Emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50722648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>[yea, note, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>[listen, charm, man, smith, __url__]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[wish, would, see, hoe, worst, part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[decad, significantli, easier, schedul, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[theim, class, =\\, alignnon, size-ful, wp-imag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text account.type  \\\n",
       "0                             YEA now that note GOOD          bot   \n",
       "1  Listen to This Charming Man by The Smiths  htt...        human   \n",
       "2  wish i can i would be seeing other hoes on the...          bot   \n",
       "3  The decade in the significantly easier schedul...          bot   \n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...          bot   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                                  [yea, note, good]  \n",
       "1               [listen, charm, man, smith, __url__]  \n",
       "2               [wish, would, see, hoe, worst, part]  \n",
       "3  [decad, significantli, easier, schedul, like, ...  \n",
       "4  [theim, class, =\\, alignnon, size-ful, wp-imag...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    twt = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    # combine stop words and punctuation\n",
    "    stop = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    \n",
    "    # create the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tokens = [ lemmatizer.lemmatize(stemmer.stem(token))\n",
    "              for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "        allText = \" \".join(tokens)\n",
    "        allText = re.sub(r'http\\S+', '__url__', allText)\n",
    "        tokens = allText.split()\n",
    "        tokenized_tweets.append(tokens)\n",
    "    return tokenized_tweets\n",
    "\n",
    "train_df['cleaned_text'] = tokenizeTweets(train_df['text'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57edaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unnecessary columns and tokenize the tweets\n",
    "val_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)\n",
    "val_df['cleaned_text'] = tokenizeTweets(val_df['text'])\n",
    "\n",
    "test_df.drop(['user_id','status_id','screen_name','class_type','screen_name_anonymized'], axis=1, inplace=True)\n",
    "test_df['cleaned_text'] = tokenizeTweets(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd29bbc",
   "metadata": {},
   "source": [
    "<h2>Modeling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the tokenzied data machine readable and categorical encoding for the labels\n",
    "x_train = train_df['cleaned_text'].tolist()\n",
    "x_train = [\" \".join(i) for i in x_train]\n",
    "\n",
    "x_val = val_df['cleaned_text'].tolist()\n",
    "x_val = [\" \".join(i) for i in x_val]\n",
    "\n",
    "x_test = test_df['cleaned_text'].tolist()\n",
    "x_test = [\" \".join(i) for i in x_test]\n",
    "\n",
    "dictLabels = {\"human\":0, \"bot\":1}\n",
    "dictLabelsReverse = {0:\"human\", 1: \"bot\"}\n",
    "\n",
    "y_train = train_df[\"account.type\"].apply(lambda x: dictLabels[x])\n",
    "y_val = val_df[\"account.type\"].apply(lambda x: dictLabels[x])\n",
    "y_test = test_df[\"account.type\"].apply(lambda x: dictLabels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15cf15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yea</th>\n",
       "      <td>0.688537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>note</th>\n",
       "      <td>0.607476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.396093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petti</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excless</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclass</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitedli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>željko</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20872 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "yea        0.688537\n",
       "note       0.607476\n",
       "good       0.396093\n",
       "00         0.000000\n",
       "petti      0.000000\n",
       "...             ...\n",
       "excless    0.000000\n",
       "exclass    0.000000\n",
       "excitedli  0.000000\n",
       "excit      0.000000\n",
       "željko     0.000000\n",
       "\n",
       "[20872 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# creating TF-IDF vocab for vectorizering the data\n",
    "vect = TfidfVectorizer(ngram_range=(1, 1), max_features=25000, \n",
    "                       dtype=np.float32)\n",
    "train_features = vect.fit_transform(x_train)\n",
    "valid_features = vect.transform(x_val)\n",
    "test_features = vect.transform(x_test)\n",
    "\n",
    "\n",
    "first_vector_tfidfvectorizer = train_features[0]\n",
    "dfVec = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vect.get_feature_names(), columns=[\"tfidf\"])\n",
    "dfVec.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8938b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y_train.tolist()\n",
    "val_labels = y_val.tolist()\n",
    "test_labels = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83c57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(estimator=SVC(),\n",
      "             param_grid=[{'C': [1, 10, 100], 'gamma': [0.001, 0.0001],\n",
      "                          'kernel': ['linear', 'rbf']}])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# fitting a support vector machine and performing gridsearch\n",
    "parameters = [{'kernel': ['linear','rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                      'C': [1, 10, 100]}]\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "tuning_results = clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce8c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72      1278\n",
      "           1       0.71      0.83      0.77      1280\n",
      "\n",
      "    accuracy                           0.75      2558\n",
      "   macro avg       0.75      0.75      0.75      2558\n",
      "weighted avg       0.75      0.75      0.75      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#printing the best parameters for SVM\n",
    "print(clf.best_params_) \n",
    "\n",
    "#generate predictions for test dataset\n",
    "grid_predictions = clf.predict(test_features)\n",
    "\n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2260355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73      1278\n",
      "           1       0.72      0.80      0.76      1280\n",
      "\n",
      "    accuracy                           0.74      2558\n",
      "   macro avg       0.75      0.74      0.74      2558\n",
      "weighted avg       0.75      0.74      0.74      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fitting a Logistic Regression and performing gridsearch\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "parameters = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, parameters)\n",
    "lr_clf.fit(train_features,train_labels)\n",
    "\n",
    "#generate predictions for test dataset\n",
    "print(lr_clf.best_params_) \n",
    "grid_predictions = lr_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46c563c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "{'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70      1278\n",
      "           1       0.69      0.82      0.75      1280\n",
      "\n",
      "    accuracy                           0.73      2558\n",
      "   macro avg       0.74      0.73      0.73      2558\n",
      "weighted avg       0.74      0.73      0.73      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fitting a Random Forest Classifier and performing gridsearch\n",
    "random_state = 523\n",
    "n_estimators = [100, 300, 500]\n",
    "max_depth = [5, 15, 30]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [2, 5, 10] \n",
    "\n",
    "parameters = {'n_estimators' : n_estimators, 'max_depth' : max_depth,  \n",
    "              'min_samples_split' : min_samples_split, \n",
    "            'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(rf.get_params().keys())\n",
    "rf_clf = GridSearchCV(rf, parameters)\n",
    "rf_clf.fit(train_features,train_labels)\n",
    "\n",
    "#generate predictions for test dataset\n",
    "print(rf_clf.best_params_) \n",
    "grid_predictions = rf_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "078b6ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3437d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,batch_first=True)\n",
    "        self.lstm_h = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        h_0 = Variable(torch.zeros( 1, x.size(0), self.hidden_size)).to('cuda')\n",
    "        c_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size)).to('cuda')\n",
    "        h_1 = Variable(torch.zeros( 1, x.size(0), self.hidden_size)).to('cuda')\n",
    "        c_1 = Variable(torch.zeros(1, x.size(0), self.hidden_size)).to('cuda')\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = self.dropout(hn)\n",
    "        output, (hn, cn) = self.lstm_h(hn, (h_1, c_1))\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.linear(hn)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e54b8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy\n",
    "\n",
    "#converting the datasets into tensors\n",
    "x_train = torch.tensor(scipy.sparse.csr_matrix.todense(train_features)).float()\n",
    "y_train = torch.tensor(train_labels)\n",
    "\n",
    "x_val = torch.tensor(scipy.sparse.csr_matrix.todense(valid_features)).float()\n",
    "y_val = torch.tensor(val_labels)\n",
    "\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(test_features)).float()\n",
    "y_test = torch.tensor(test_labels)\n",
    "\n",
    "#loading the dataset and dataloader\n",
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self,x,y):\n",
    "        self.n_samples = x.shape[0]\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "train_dataset = TweetDataset(x_train,y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "val_dataset = TweetDataset(x_val,y_val)\n",
    "valid_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "test_dataset = TweetDataset(x_test,y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01feb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "\n",
    "# initialize the hyperparameters\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "\n",
    "def fitLSTM():\n",
    "    train_losses = []\n",
    "    train_accu = []\n",
    "    eval_losses = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTM(input_size, hidden_size, num_classes).to(device)\n",
    "    \n",
    "    # initialize the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            correct=0\n",
    "            total=0\n",
    "            running_loss=0\n",
    "            \n",
    "            # Train the model\n",
    "            for (words, labels) in train_loader:\n",
    "                words = words.to(device)\n",
    "                labels = labels.to(dtype=torch.long).to(device)\n",
    "                \n",
    "                # Forward prop\n",
    "                outputs = model(words)\n",
    "                \n",
    "                # Backward prop and optimize\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                running_accuracy = 0 \n",
    "                total = 0 \n",
    "                \n",
    "            # predict accuracy and store the train loss\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_loss=running_loss/len(train_loader)\n",
    "            accu=100.*correct/total\n",
    "\n",
    "            train_accu.append(accu)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validate the model\n",
    "            running_loss = 0.0\n",
    "            model.eval()     \n",
    "            for data, labels in valid_loader:\n",
    "                data = data.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                # Forward prop\n",
    "                target = model(data.float())\n",
    "                \n",
    "                # Calculate Loss\n",
    "                loss = criterion(target,labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            #store the valid loss\n",
    "            valid_loss = running_loss/len(valid_loader)\n",
    "            eval_losses.append(valid_loss)\n",
    "            \n",
    "            if min_valid_loss > valid_loss:\n",
    "                min_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            \n",
    "            if (epoch+1) % 1 == 0:\n",
    "                print('For Epoch %d ,Loss on Train set: %f' % (epoch+1, train_loss)) \n",
    "                print('For Epoch %d, Loss on Validation set: %f ' % (epoch+1, valid_loss))\n",
    "\n",
    "    # Test the model\n",
    "    path = \"saved_model.pth\" \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "     \n",
    "    running_accuracy = 0 \n",
    "    total = 0 \n",
    "    pred = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for inputs, outputs in test_loader: \n",
    "            inputs = inputs.cuda()\n",
    "            outputs = outputs.cuda()\n",
    "            \n",
    "            # Calculate Accuracy\n",
    "            outputs = outputs.to(torch.float32) \n",
    "            predicted_outputs = model(inputs.float())\n",
    "            _, predicted = torch.max(predicted_outputs, 1)\n",
    "            pred.append(predicted.cpu())\n",
    "            total += outputs.size(0)\n",
    "            running_accuracy += (predicted == outputs).sum().item()\n",
    "            \n",
    "        #print the classification report for precision, recall and F1 score\n",
    "        pred = torch.cat(pred)\n",
    "        print(classification_report(pred,y_test))\n",
    "\n",
    "\n",
    "    print('Accuracy of the model based on the test set of inputs is: %d %%' % (100 * running_accuracy / total))    \n",
    "    \n",
    "    # print the accuracy by label\n",
    "    labels_length = 2\n",
    "    labels_correct =[0. for i in range(labels_length)]  \n",
    "    labels_total = [0. for i in range(labels_length)]\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "         for inputs, outputs in test_loader: \n",
    "            inputs = inputs.cuda()\n",
    "            outputs = outputs.cuda()\n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "             \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == outputs[i]:\n",
    "                    labels_correct[outputs[i]] += 1\n",
    "                labels_total[outputs[i]] += 1\n",
    "                \n",
    "    for i in range(2):\n",
    "        print('Accuracy to predict %5s : %2d %%' % (i, 100 * labels_correct[i] / labels_total[i]))\n",
    "        plot_epochs = [i for i in range(num_epochs)]\n",
    "    \n",
    "    # plot the losses per epoch\n",
    "    plt.plot(plot_epochs,train_losses,label=\"Train Loss\",color=\"blue\")\n",
    "    plt.plot(plot_epochs,eval_losses,label=\"Val Loss\",color=\"green\")\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('tfidf_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc2ba049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epoch 1 ,Loss on Train set: 0.626462\n",
      "For Epoch 1, Loss on Validation set: 0.559458 \n",
      "For Epoch 2 ,Loss on Train set: 0.534716\n",
      "For Epoch 2, Loss on Validation set: 0.550228 \n",
      "For Epoch 3 ,Loss on Train set: 0.516961\n",
      "For Epoch 3, Loss on Validation set: 0.549210 \n",
      "For Epoch 4 ,Loss on Train set: 0.508312\n",
      "For Epoch 4, Loss on Validation set: 0.552140 \n",
      "For Epoch 5 ,Loss on Train set: 0.502566\n",
      "For Epoch 5, Loss on Validation set: 0.556033 \n",
      "For Epoch 6 ,Loss on Train set: 0.499966\n",
      "For Epoch 6, Loss on Validation set: 0.555646 \n",
      "For Epoch 7 ,Loss on Train set: 0.497913\n",
      "For Epoch 7, Loss on Validation set: 0.558504 \n",
      "For Epoch 8 ,Loss on Train set: 0.496779\n",
      "For Epoch 8, Loss on Validation set: 0.561846 \n",
      "For Epoch 9 ,Loss on Train set: 0.496652\n",
      "For Epoch 9, Loss on Validation set: 0.564385 \n",
      "For Epoch 10 ,Loss on Train set: 0.495952\n",
      "For Epoch 10, Loss on Validation set: 0.562298 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.50      0.48      1162\n",
      "           1       0.55      0.50      0.53      1396\n",
      "\n",
      "    accuracy                           0.50      2558\n",
      "   macro avg       0.50      0.50      0.50      2558\n",
      "weighted avg       0.51      0.50      0.50      2558\n",
      "\n",
      "Accuracy of the model based on the test set of inputs is: 73 %\n",
      "Accuracy to predict     0 : 69 %\n",
      "Accuracy to predict     1 : 78 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07065cb7",
   "metadata": {},
   "source": [
    "<h1> BERT Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d986ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->1000\n",
      "->2000\n",
      "->3000\n",
      "->4000\n",
      "->5000\n",
      "->6000\n",
      "->7000\n",
      "->8000\n",
      "->9000\n",
      "->10000\n",
      "->11000\n",
      "->12000\n",
      "->13000\n",
      "->14000\n",
      "->15000\n",
      "->16000\n",
      "->17000\n",
      "->18000\n",
      "->19000\n",
      "->20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->1000\n",
      "->2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->1000\n",
      "->2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "\n",
    "# create a function to tokenize the text to BERT embeddings    \n",
    "def BERTtokenizeTweets(df):\n",
    "    pretrained_weights = \"bert-base-cased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = BertModel.from_pretrained(pretrained_weights)\n",
    "    nlp = pipeline(\"feature-extraction\", tokenizer=tokenizer, model=model)\n",
    "    vectorized_docs = []\n",
    "    count = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        vec = np.array(nlp(row[\"text\"][:512]))\n",
    "        meanVec = vec.reshape((vec.shape[1], vec.shape[2])).mean(axis=0)\n",
    "        vectorized_docs.append(meanVec)\n",
    "        count = count + 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"->\"+str(count)) \n",
    "\n",
    "    return np.array(vectorized_docs)\n",
    "\n",
    "# convert the data to BERT tokenized text\n",
    "train_features = BERTtokenizeTweets(train_df)\n",
    "val_features = BERTtokenizeTweets(val_df)\n",
    "test_features = BERTtokenizeTweets(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fb1bd2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      1278\n",
      "           1       0.83      0.86      0.85      1280\n",
      "\n",
      "    accuracy                           0.84      2558\n",
      "   macro avg       0.84      0.84      0.84      2558\n",
      "weighted avg       0.84      0.84      0.84      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# fitting a support vector machine and performing gridsearch\n",
    "parameters = [{'kernel': ['linear','rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                      'C': [1, 10, 100]}]\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "tuning_results = clf.fit(train_features,train_labels)\n",
    "\n",
    "#printing the best parameters for SVM\n",
    "print(clf.best_params_)\n",
    "\n",
    "#generate predictions for test dataset\n",
    "grid_predictions = clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6478c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84      1278\n",
      "           1       0.83      0.85      0.84      1280\n",
      "\n",
      "    accuracy                           0.84      2558\n",
      "   macro avg       0.84      0.84      0.84      2558\n",
      "weighted avg       0.84      0.84      0.84      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fitting a logistic regression and performing gridsearch\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "parameters = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "\n",
    "lr_clf = GridSearchCV(lr, parameters)\n",
    "lr_clf.fit(train_features,train_labels)\n",
    "\n",
    "#printing the best parameters for LR\n",
    "print(lr_clf.best_params_) \n",
    "\n",
    "#generate predictions for test dataset\n",
    "grid_predictions = lr_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4b04b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "{'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82      1278\n",
      "           1       0.80      0.87      0.83      1280\n",
      "\n",
      "    accuracy                           0.83      2558\n",
      "   macro avg       0.83      0.83      0.82      2558\n",
      "weighted avg       0.83      0.83      0.82      2558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fitting a Random Forest Classifier and performing gridsearch\n",
    "random_state = 523\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [15, 30]\n",
    "min_samples_split = [10, 15]\n",
    "min_samples_leaf = [ 5, 10] \n",
    "\n",
    "parameters = {'n_estimators' : n_estimators, 'max_depth' : max_depth,  \n",
    "              'min_samples_split' : min_samples_split, \n",
    "            'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(rf.get_params().keys())\n",
    "rf_clf = GridSearchCV(rf, parameters)\n",
    "rf_clf.fit(train_features,train_labels)\n",
    "\n",
    "#printing the best parameters for RF\n",
    "print(rf_clf.best_params_)\n",
    "\n",
    "#generate predictions for test dataset\n",
    "grid_predictions = rf_clf.predict(test_features) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(test_labels, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd3d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy\n",
    "\n",
    "#loading the dataset and dataloader\n",
    "\n",
    "BERTtrain_dataset = TweetDataset(train_features,y_train)\n",
    "BERTtrain_loader = DataLoader(dataset = BERTtrain_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "BERTval_dataset = TweetDataset(val_features,y_val)\n",
    "BERTvalid_loader = DataLoader(dataset = BERTval_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "BERTtest_dataset = TweetDataset(test_features,y_test)\n",
    "BERTtest_loader = DataLoader(dataset = BERTtest_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5225941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def fitBERTLSTM():\n",
    "    # initialize the hyperparameters\n",
    "    train_losses=[]\n",
    "    train_accu=[]\n",
    "    eval_losses = []\n",
    "    num_epochs = 10\n",
    "    start_time = time.time()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTM(input_size, hidden_size, num_classes).to(device)\n",
    "    \n",
    "    # initialize the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        correct=0\n",
    "        total=0\n",
    "        running_loss=0\n",
    "        for (words, labels) in BERTtrain_loader:\n",
    "            words = words.to(device)\n",
    "            labels = labels.to(dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward prop\n",
    "            outputs = model(words)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward prop and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            running_accuracy = 0 \n",
    "            total = 0\n",
    "            \n",
    "        # calculate accuracy and store the train loss\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss=running_loss/len(BERTtrain_loader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate the model\n",
    "        running_loss = 0.0\n",
    "        model.eval()     \n",
    "        for data, labels in BERTvalid_loader:\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward Pass\n",
    "            target = model(data.float())\n",
    "            # Find the Loss\n",
    "            loss = criterion(target,labels)\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        #store the valid loss\n",
    "        valid_loss = running_loss/len(BERTvalid_loader)\n",
    "        eval_losses.append(valid_loss)\n",
    "        \n",
    "        if min_valid_loss > valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_model_bert.pth')\n",
    "            \n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print('For Epoch %d ,Loss on Train set: %f' % (epoch+1, train_loss)) \n",
    "            print('For Epoch %d, Loss on Validation set: %f' % (epoch+1, valid_loss))\n",
    "        \n",
    "    \n",
    "    \n",
    "    # print the training time\n",
    "    training_time = time.time()-start_time\n",
    "    print(\"Training time: {}\".format(training_time))\n",
    "    \n",
    "    # Test the model\n",
    "    path = \"saved_model_bert.pth\" \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "    \n",
    "    running_accuracy = 0 \n",
    "    total = 0 \n",
    "    pred = []\n",
    "    with torch.no_grad(): \n",
    "        for inputs, outputs in BERTtest_loader: \n",
    "            inputs = inputs.cuda()\n",
    "            outputs = outputs.cuda()\n",
    "            outputs = outputs.to(torch.float32) \n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1)\n",
    "            pred.append(predicted.cpu())\n",
    "            total += outputs.size(0) \n",
    "            running_accuracy += (predicted == outputs).sum().item()\n",
    "            \n",
    "        #print the classification report for precision, recall and F1 score\n",
    "        pred = torch.cat(pred)\n",
    "        print(classification_report(pred,y_test))\n",
    " \n",
    "        \n",
    "    print('Accuracy of the model based on the test set of inputs is: %d %%' % (100 * running_accuracy / total))\n",
    "    \n",
    "    # print the accuracy by label\n",
    "    labels_length = 2\n",
    "    labels_correct =[0. for i in range(labels_length)] \n",
    "    labels_total = [0. for i in range(labels_length)]\n",
    "  \n",
    "    with torch.no_grad(): \n",
    "         for inputs, outputs in BERTtest_loader: \n",
    "            inputs = inputs.cuda()\n",
    "            outputs = outputs.cuda()\n",
    "            predicted_outputs = model(inputs.float()) \n",
    "            _, predicted = torch.max(predicted_outputs, 1) \n",
    "             \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == outputs[i]:\n",
    "                    labels_correct[outputs[i]] += 1\n",
    "                labels_total[outputs[i]] += 1\n",
    "\n",
    "    \n",
    "    for i in range(2):\n",
    "        print('Accuracy to predict %5s : %2d %%' % (i, 100 * labels_correct[i] / labels_total[i]))\n",
    "        \n",
    "    # plot the losses per epoch    \n",
    "    plot_epochs = [i for i in range(num_epochs)]\n",
    "    plt.plot(plot_epochs,train_losses,label=\"Train Loss\",color=\"blue\")\n",
    "    plt.plot(plot_epochs,eval_losses,label=\"Val Loss\",color=\"green\")\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('bert_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eefc0709",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20872, got 768",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfitBERTLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mfitBERTLSTM\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Forward prop\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward prop and optimize\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m h_1 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros( \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m c_1 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m hn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hn)\n\u001b[0;32m     21\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_h(hn, (h_1, c_1))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:689\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 689\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    692\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:632\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    628\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    629\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    630\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    631\u001b[0m                        ):\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    634\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    636\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20872, got 768"
     ]
    }
   ],
   "source": [
    "fitBERTLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbc86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
